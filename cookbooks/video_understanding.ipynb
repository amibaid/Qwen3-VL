{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "07a5e83c",
      "metadata": {
        "id": "07a5e83c"
      },
      "source": [
        "### Video Understanding with Qwen3-VL\n",
        "\n",
        "In this notebook, we delve into the capabilities of the **Qwen3-VL** model for video understanding tasks. Our objective is to showcase how this advanced model can be applied to various video analysis scenarios.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "223b154d",
      "metadata": {
        "id": "223b154d"
      },
      "source": [
        "#### \\[Setup\\]\n",
        "\n",
        "We start by loading the pre-trained `Qwen3-VL` model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5623f17b",
      "metadata": {
        "ExecutionIndicator": {
          "show": false
        },
        "tags": [],
        "id": "5623f17b",
        "outputId": "50b46f48-2ec5-44c3-a2ad-34d3ba66d399"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "`torch_dtype` is deprecated! Use `dtype` instead!\n",
            "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.69it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "output_loading_info {'missing_keys': [], 'unexpected_keys': [], 'mismatched_keys': [], 'error_msgs': []}\n"
          ]
        }
      ],
      "source": [
        "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
        "from qwen_vl_utils import process_vision_info\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning, module=\"transformers\")\n",
        "import os\n",
        "\n",
        "model_path = \"Qwen/Qwen3-VL-8B-Instruct\"\n",
        "processor = AutoProcessor.from_pretrained(model_path)\n",
        "\n",
        "model, output_loading_info = AutoModelForVision2Seq.from_pretrained(model_path, torch_dtype=\"auto\", device_map=\"auto\", output_loading_info=True)\n",
        "print(\"output_loading_info\", output_loading_info)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab78f266",
      "metadata": {
        "id": "ab78f266"
      },
      "source": [
        "Load video frames and timestamps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f08173dc",
      "metadata": {
        "id": "f08173dc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import hashlib\n",
        "import requests\n",
        "\n",
        "from IPython.display import Markdown, display\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import decord\n",
        "from decord import VideoReader, cpu\n",
        "\n",
        "\n",
        "def download_video(url, dest_path):\n",
        "    response = requests.get(url, stream=True)\n",
        "    with open(dest_path, 'wb') as f:\n",
        "        for chunk in response.iter_content(chunk_size=8096):\n",
        "            f.write(chunk)\n",
        "    print(f\"Video downloaded to {dest_path}\")\n",
        "\n",
        "\n",
        "def get_video_frames(video_path, num_frames=128, cache_dir='.cache'):\n",
        "    os.makedirs(cache_dir, exist_ok=True)\n",
        "\n",
        "    video_hash = hashlib.md5(video_path.encode('utf-8')).hexdigest()\n",
        "    if video_path.startswith('http://') or video_path.startswith('https://'):\n",
        "        video_file_path = os.path.join(cache_dir, f'{video_hash}.mp4')\n",
        "        if not os.path.exists(video_file_path):\n",
        "            download_video(video_path, video_file_path)\n",
        "    else:\n",
        "        video_file_path = video_path\n",
        "\n",
        "    frames_cache_file = os.path.join(cache_dir, f'{video_hash}_{num_frames}_frames.npy')\n",
        "    timestamps_cache_file = os.path.join(cache_dir, f'{video_hash}_{num_frames}_timestamps.npy')\n",
        "\n",
        "    if os.path.exists(frames_cache_file) and os.path.exists(timestamps_cache_file):\n",
        "        frames = np.load(frames_cache_file)\n",
        "        timestamps = np.load(timestamps_cache_file)\n",
        "        return video_file_path, frames, timestamps\n",
        "\n",
        "    vr = VideoReader(video_file_path, ctx=cpu(0))\n",
        "    total_frames = len(vr)\n",
        "\n",
        "    indices = np.linspace(0, total_frames - 1, num=num_frames, dtype=int)\n",
        "    frames = vr.get_batch(indices).asnumpy()\n",
        "    timestamps = np.array([vr.get_frame_timestamp(idx) for idx in indices])\n",
        "\n",
        "    np.save(frames_cache_file, frames)\n",
        "    np.save(timestamps_cache_file, timestamps)\n",
        "\n",
        "    return video_file_path, frames, timestamps\n",
        "\n",
        "\n",
        "def create_image_grid(images, num_columns=8):\n",
        "    pil_images = [Image.fromarray(image) for image in images]\n",
        "    num_rows = math.ceil(len(images) / num_columns)\n",
        "\n",
        "    img_width, img_height = pil_images[0].size\n",
        "    grid_width = num_columns * img_width\n",
        "    grid_height = num_rows * img_height\n",
        "    grid_image = Image.new('RGB', (grid_width, grid_height))\n",
        "\n",
        "    for idx, image in enumerate(pil_images):\n",
        "        row_idx = idx // num_columns\n",
        "        col_idx = idx % num_columns\n",
        "        position = (col_idx * img_width, row_idx * img_height)\n",
        "        grid_image.paste(image, position)\n",
        "\n",
        "    return grid_image\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e6ffbbc2",
      "metadata": {
        "id": "e6ffbbc2"
      },
      "source": [
        "Inference function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ef59a8c",
      "metadata": {
        "id": "2ef59a8c"
      },
      "outputs": [],
      "source": [
        "def inference(video, prompt, max_new_tokens=2048, total_pixels=20480 * 32 * 32, min_pixels=64 * 32 * 32, max_frames= 2048, sample_fps = 2):\n",
        "    \"\"\"\n",
        "    Perform multimodal inference on input video and text prompt to generate model response.\n",
        "\n",
        "    Args:\n",
        "        video (str or list/tuple): Video input, supports two formats:\n",
        "            - str: Path or URL to a video file. The function will automatically read and sample frames.\n",
        "            - list/tuple: Pre-sampled list of video frames (PIL.Image or url).\n",
        "              In this case, `sample_fps` indicates the frame rate at which these frames were sampled from the original video.\n",
        "        prompt (str): User text prompt to guide the model's generation.\n",
        "        max_new_tokens (int, optional): Maximum number of tokens to generate. Default is 2048.\n",
        "        total_pixels (int, optional): Maximum total pixels for video frame resizing (upper bound). Default is 20480*32*32.\n",
        "        min_pixels (int, optional): Minimum total pixels for video frame resizing (lower bound). Default is 16*32*32.\n",
        "        sample_fps (int, optional): ONLY effective when `video` is a list/tuple of frames!\n",
        "            Specifies the original sampling frame rate (FPS) from which the frame list was extracted.\n",
        "            Used for temporal alignment or normalization in the model. Default is 2.\n",
        "\n",
        "    Returns:\n",
        "        str: Generated text response from the model.\n",
        "\n",
        "    Notes:\n",
        "        - When `video` is a string (path/URL), `sample_fps` is ignored and will be overridden by the video reader backend.\n",
        "        - When `video` is a frame list, `sample_fps` informs the model of the original sampling rate to help understand temporal density.\n",
        "    \"\"\"\n",
        "\n",
        "    messages = [\n",
        "        {\"role\": \"user\", \"content\": [\n",
        "                {\"video\": video,\n",
        "                \"total_pixels\": total_pixels,\n",
        "                \"min_pixels\": min_pixels,\n",
        "                \"max_frames\": max_frames,\n",
        "                'sample_fps':sample_fps},\n",
        "                {\"type\": \"text\", \"text\": prompt},\n",
        "            ]\n",
        "        },\n",
        "    ]\n",
        "    text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    image_inputs, video_inputs, video_kwargs = process_vision_info([messages], return_video_kwargs=True,\n",
        "                                                                   image_patch_size= 16,\n",
        "                                                                   return_video_metadata=True)\n",
        "    if video_inputs is not None:\n",
        "        video_inputs, video_metadatas = zip(*video_inputs)\n",
        "        video_inputs, video_metadatas = list(video_inputs), list(video_metadatas)\n",
        "    else:\n",
        "        video_metadatas = None\n",
        "    inputs = processor(text=[text], images=image_inputs, videos=video_inputs, video_metadata=video_metadatas, **video_kwargs, do_resize=False, return_tensors=\"pt\")\n",
        "    inputs = inputs.to('cuda')\n",
        "\n",
        "    output_ids = model.generate(**inputs, max_new_tokens=max_new_tokens)\n",
        "    generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n",
        "    output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
        "    return output_text[0]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58729648",
      "metadata": {
        "id": "58729648"
      },
      "source": [
        "####  \\[Usage\\]\n",
        "\n",
        "Once the model is loaded (or the API is ready), you can provide video inputs in **two formats**:\n",
        "\n",
        "1. **Video URL (`video_url`)** — A file path or publicly accessible HTTP(S) URL pointing to a video file (e.g., MP4, AVI).  \n",
        "   ✅ Best for quick prototyping or when you want the model/API to handle video decoding internally.\n",
        "2. **Frame List (`frame_list`)** — A list of PIL Image objects or file paths representing sampled frames from a video.  \n",
        "   ✅ Best for fine-grained control, preprocessing, or when you’ve already decoded the video.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ee84f04",
      "metadata": {
        "id": "1ee84f04"
      },
      "source": [
        "### 3. Using Frame List — Local Inference"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43741aa7",
      "metadata": {
        "id": "43741aa7",
        "outputId": "fbfa8a50-f3d9-4d6e-c0c3-c1a0b1224263"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "A woman in a kitchen prepares and cooks a dish of meat, onions, and cheese wrapped in dough, which she then fries until golden brown. The final product is served with ketchup and garnished with parsley."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "## 3. API Inference — Using Video URL  Apply API key here: https://bailian.console.alibabacloud.com/?apiKey=1\n",
        "# Base URL for pre-extracted video frames (public OSS bucket)\n",
        "video_frame_dir = 'https://ofasys-multimodal-wlcb-3-toshanghai.oss-cn-shanghai.aliyuncs.com/Qwen3VL/demo/video/demo_cooking'\n",
        "\n",
        "# Configure sampling: e.g., 0.25 FPS = 1 frame per 4 seconds\n",
        "sample_fps=0.25 # or =1\n",
        "\n",
        "# The maximum number of pixels expected to be used from the video — adjustable based on available GPU memory. Our model natively supports up to 256K input tokens.\n",
        "total_pixels=24*1024*32*32   # or 256*1024*32*32\n",
        "\n",
        "video_frame_list = [f\"{video_frame_dir}/{i}.000.jpg\" for i in range(0, 1228, int(1/sample_fps))]\n",
        "prompt = \"Briefly describe the video.\"\n",
        "response = inference(video_frame_list, prompt, sample_fps=sample_fps, total_pixels=total_pixels)\n",
        "display(Markdown(response))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6c47b35f",
      "metadata": {
        "id": "6c47b35f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}